# Enhanced AI & Digital Transformation Readiness Assessment (Nonprofits – 2025 Edition)

Overview: This revised self-assessment is organized into five key categories covering all facets of AI and digital readiness: Strategy & Governance, Data & Technology Infrastructure, People & Skills, Process & Data Practices, and AI Applications & Risks[1]. It blends quantitative rating-scale questions (scored 0–10) with a few qualitative prompts to capture context. Each question is phrased as a full, reflective prompt (not just a single word or phrase) grounded in core nonprofit domains – strategy, operations, governance, data use, ethics, technology infrastructure, and AI application readiness[2][3]. This design draws on latest best practices (e.g. data.org's Purpose-Practice-People framework and AI section[4], OECD's multi-pillar AI readiness indicators, and nonprofit AI maturity models) to ensure comprehensive coverage of factors like leadership support, data quality, staff skills, and ethical safeguards[5][2]. The assessment is intended for use in an interactive web tool, providing real-time scoring and feedback. Questions are written in plain, accessible language with minimal jargon and examples for clarity[6], so that both nonprofit staff and external facilitators can use it easily. The tone is supportive and self-reflective – emphasizing learning and improvement rather than judgment[7].

## Survey Questions by Category

Below is the full list of survey questions, grouped by category. Each question includes its response type, scoring guidance, and category mapping (for scoring). Quantitative items use a 0–10 scoring scale (e.g. Likert-style ratings or converted multiple-choice options), where higher scores indicate greater readiness. Qualitative items are open-ended; for these, guidance is provided on how to code or interpret responses (e.g. what to look for in answers) so evaluators can assign a low/medium/high score or note key themes. Note: Some questions employ conditional branching – these are marked accordingly. All categories are important to overall readiness, though weights differ slightly (see Scoring Model).

### Strategy & Governance

Assesses leadership buy-in, strategic planning, and ethical governance practices around data and AI[3]. This category examines how well the organization's leaders prioritize digital transformation and AI in service of the mission, the presence of strategy and policy frameworks, and the ethical guidelines steering technology use.

**Does your organization have a documented digital transformation or AI strategy that is actively supported by leadership?**

Response Type: Likert scale (0–10) – 0 = "No strategy at all, and leadership has shown little support for digital initiatives"; 10 = "Yes, we have a clear written digital/AI strategy endorsed and regularly communicated by leadership."

Scoring Guidance: Score the response from 0 (no strategy/vision) up to 10 (well-defined strategy with full leadership buy-in). Intermediate ratings (e.g. 5) indicate partial or informal strategy.

Category Mapping: Strategy & Governance (high weight in overall score, as leadership and clear strategy are critical drivers[8]).

**Has your leadership (executive team and board) established policies or guidelines for responsible data use and ethical AI (e.g. addressing data privacy, bias, transparency)?**

Response Type: Multiple-choice (single answer) – options: "No policies or guidelines at all"; "Not yet, but we are developing them"; "Yes, we have basic guidelines"; "Yes, we have comprehensive written policies." (These can be mapped to 0, 3, 7, 10 points respectively.)

Scoring Guidance: Assign 0 if no guidelines; assign mid-range (3–7) if in development or basic informal guidelines; assign 10 for formal, comprehensive policies in place.

Category Mapping: Strategy & Governance (policies for ethical AI/data governance are a key strategic indicator[9]).

**Have resources (budget or dedicated funding) been allocated specifically for technology, data, or AI initiatives in your organization?**

Response Type: Likert scale (0–10) – 0 = "No dedicated budget for digital/AI (only ad-hoc or none)"; 5 = "Some occasional or one-time funding for tech projects"; 10 = "Yes, a stable dedicated budget or fund exists for digital innovation/AI initiatives."

Scoring Guidance: Higher scores reflect more committed resource allocation. Score 0 if there is no specific budget (all tech efforts are unfunded or purely volunteer-based); score mid-range for small or sporadic funds; score 10 if a substantial, recurring budget line or grant supports digital transformation efforts.

Category Mapping: Strategy & Governance (financial commitment is an aspect of strategic priority).

**[Open-Ended] In your own words, how do organizational leaders perceive the importance of digital transformation and AI? (E.g. "Describe any statements, plans, or actions from leadership regarding adopting new technology or data-driven approaches.")**

Response Type: Open-ended text.

Scoring Guidance: Qualitative coding: Look for indications of leadership's mindset and actions. For a high maturity response, expect concrete examples (e.g. leadership has formally outlined a digital vision, set goals or formed an innovation committee) – this would warrant a higher score (e.g. 8–10). A moderate response might mention general positive attitude but no formal plans (score ~4–7). A low response might reveal leadership is uninvolved, skeptical, or unaware of digital issues (score ~0–3). If scoring is not automated, use this response to contextualize the Strategy score and identify leadership attitudes or misconceptions.

Category Mapping: Strategy & Governance (qualitative insight to supplement the quantitative scores in this category).

### Data & Technology Infrastructure

Evaluates the technical backbone for data and AI projects, including data quality, availability, and IT infrastructure (software, hardware, connectivity)[10]. This category checks whether the organization's technology environment can support analytics or AI – covering data management practices, systems integration, and basic infrastructure like connectivity and tools.

**How would you describe the quality and accessibility of your organization's data infrastructure?(For example, do you have centralized, clean, and secure data repositories that staff can easily access for analysis?)[11]**

Response Type: Likert scale (0–10) – 0 = "Data is mostly siloed, poor quality, or not stored digitally in a reliable way"; 5 = "Some data systems in place, but data may be inconsistent or not easily accessible across the organization"; 10 = "High-quality, centralized databases or data warehouses exist with clean, well-organized data that staff can securely access as needed."

Scoring Guidance: Score based on the completeness of data infrastructure. Low scores (0–3) if data is largely ad-hoc (e.g. spreadsheets on individual computers) with major quality issues; mid-range for partial systems or significant issues with integration/cleanliness; high (8–10) if the organization has a robust, centralized and secure data system.

Category Mapping: Data & Technology Infrastructure (core data backbone readiness, aligned with the "Data" pillar in maturity frameworks[12]).

**What technology tools and platforms are in use to support your programs and operations?(Examples: donor or CRM databases, monitoring & evaluation systems, cloud analytics platforms, etc.)**

Response Type: Multiple-choice (check all that apply, with a follow-up rating) – e.g.: "Mostly basic office software (documents, spreadsheets) with no specialized data systems"; "Some dedicated tools for specific needs (e.g. a donor database or accounting software)"; "Multiple integrated platforms (CRM, data analytics tools, etc.) in use"; "Advanced infrastructure (cloud platforms, enterprise systems, AI/ML tools) in place."

Scoring Guidance: This can be scored on a 0–10 scale by mapping the most advanced option selected. If only basic tools are used, score ~0–2. If some dedicated systems exist, score in mid-range (~4–6). If several systems and some integration are present, score higher (~7–8). If advanced, scalable tech (cloud, AI tools) is in place, score ~9–10.

Category Mapping: Data & Technology Infrastructure (gauges IT tool availability and sophistication to support data/AI work).

**Do staff have reliable access to the necessary hardware, software, and internet connectivity to effectively use data and digital tools?**

Response Type: Likert scale (0–10) – 0 = "No, there are significant gaps (e.g. many staff lack computers or stable internet)"; 5 = "Partially – most have basic access, but reliability or software capabilities are limited"; 10 = "Yes, all staff who need it have reliable devices, up-to-date software, and high-speed internet connectivity."

Scoring Guidance: Score lower if technological access is a barrier (e.g. outdated equipment, poor internet for many staff); score mid if access is generally available but with some limitations; score high if technology access is universally reliable, enabling digital work.

Category Mapping: Data & Technology Infrastructure (basic IT readiness, a prerequisite for digital projects).

**How well integrated are your data and technology systems across the organization's departments/functions?**

Response Type: Likert scale (0–10) – 0 = "Not at all – systems are siloed and don't talk to each other, requiring manual data transfers"; 5 = "Somewhat – a few systems are integrated or share data, but important gaps remain"; 10 = "Highly integrated – our systems (e.g. program database, CRM, finance, etc.) are connected or centralized, allowing seamless data flow across the organization."

Scoring Guidance: Higher scores reflect more integration. Score 0 for completely siloed systems, 5 for partial integration (some links or common platforms), and 10 for a well-integrated or single-platform approach. Integration is crucial for data availability and breaking down silos.

Category Mapping: Data & Technology Infrastructure (systems integration aspect, related to data accessibility in the practice of using data[13]).

(No open-ended question in this category to keep the survey brief; quantitative items above sufficiently indicate infrastructure status.)

### People & Skills

Focuses on staff capacity, skills, and culture for data/AI innovation[14]. This category examines the human side: the team's data literacy and AI skills, training efforts, presence of data/technology champions or experts (internal or external), and the organizational culture around innovation and change.

**What is the overall level of data literacy and AI knowledge among your staff and volunteers?**

Response Type: Likert scale (0–10) – 0 = "Very low – most of our team is unfamiliar with data analysis or AI concepts"; 5 = "Moderate – some staff have basic data skills or awareness of AI, but it's not widespread"; 10 = "High – many staff are comfortable with data, and at least a few have substantial expertise in data science/AI. We regularly refresh these skills through learning."

Scoring Guidance: Score based on how widespread and deep data/AI skills are. Low (0–2) if almost no capability; mid (4–6) if there is a mix of basic skills; high (8–10) if strong skills are common or there are dedicated experts. (This reflects whether the organization has the people capacity for AI, identified as a key factor in readiness[8].)

Category Mapping: People & Skills.

**Do you offer (or facilitate access to) training or professional development in data or digital skills for your staff?**

Response Type: Multiple-choice – options: "No, there are currently no training opportunities in data/tech provided"; "Not formally, but we occasionally share resources or encourage self-learning"; "Yes, we have some training programs or send staff to workshops occasionally"; "Yes, we have a regular, structured training/upskilling program for data and AI." (Map to 0, 3, 7, 10 points.)

Scoring Guidance: 0 if no training support; low-mid (3) if only informal encouragement; 7 if some training happens on an ad-hoc basis; 10 if a clear ongoing program or budget for staff training in data/tech is in place.

Category Mapping: People & Skills (investment in building staff skills).

**Is there a designated person or team responsible for data and technology (e.g. a "data analyst", IT lead, or digital innovation champion)?**

Response Type: Multiple-choice – "No, these responsibilities are ad-hoc or added on to everyone's duties" (score 0); "Not officially, but there are one or two tech-savvy people who informally take the lead" (score ~5); "Yes, we have at least one staff role (or a specific volunteer/consultant) focused on data/technology" (score ~8); "Yes, we have a formal team or department for IT/data/analytics" (score 10).

Scoring Guidance: Score higher for more formalized roles or teams. The presence of a dedicated "data champion" indicates organizational commitment to digital practices[14].

Category Mapping: People & Skills.

**How would you characterize your organizational culture when it comes to technology and innovation?**

Response Type: Likert scale (0–10) – 0 = "Change-averse – staff generally stick to traditional methods and there is skepticism or fear around new technology"; 5 = "Neutral/mixed – some openness to new tools, but also some resistance or lack of time to innovate"; 10 = "Innovation-friendly – leadership encourages trying new digital tools, cross-team collaboration is common, and learning from failures is supported."

Scoring Guidance: Score on a continuum of culture. Low scores for rigid, change-resistant environments; mid for partially open but not fully embracing change; high for proactive, forward-looking cultures that celebrate experimentation (since a supportive culture is known to influence AI readiness[8]).

Category Mapping: People & Skills.

**[Open-Ended] What steps (if any) has your organization taken to build staff capacity in data or AI, and what gaps remain?E.g. "Describe any training attended, new hires/volunteers brought on, or internal initiatives to improve data skills, as well as areas where more support is needed."**

Response Type: Open-ended text.

Scoring Guidance: Qualitative analysis: Look for evidence of proactive skill-building and remaining needs. A high maturity answer might list specific initiatives (trainings completed, data experts hired, peer learning groups formed) – showing strategic skill development (score toward 8–10). A moderate answer might indicate some efforts (one-off training or intent to train) but also gaps (score ~4–7). A low answer could be, for example, "No significant efforts yet, we know we lack data skills" (score ~0–3). Use this to identify patterns: e.g. whether the nonprofit leverages external support (volunteer analysts, partnerships) or is unaware of skill needs. This context can guide tailored recommendations (like training resources) in the report.

Category Mapping: People & Skills.

### Process & Data Practices

Examines how well the organization integrates digital tools and data into everyday workflows and decision-making[15]. This category looks at operational processes: Are programs and services managed using modern systems that generate useful data? Is decision-making data-informed? Are processes standardized enough to leverage technology effectively, or are they ad-hoc?

**To what extent are your programmatic and operational workflows digitized or managed through software systems (as opposed to being manual/paper-based)?(Examples: using a CRM for donor management, an online M&E system for program tracking, digital accounting or HR systems, etc.)[16]**

Response Type: Likert scale (0–10) – 0 = "Not at all – most tasks are done offline or with basic tools like spreadsheets, and there is little use of specialized software"; 5 = "Somewhat – we use digital tools in a few areas (e.g. donor database or accounting software), but many processes are still manual or on disparate systems"; 10 = "Extensively – the majority of our operations (program management, fundraising, admin) use dedicated software or online platforms, minimizing manual work and generating data."

Scoring Guidance: Higher scores mean more fully digitized processes. Score very low if the organization is largely paper-based or using only generic tools; mid if a mix of manual and digital processes; high if most processes have been converted to digital systems (important for generating data to leverage in AI).

Category Mapping: Process & Data Practices (captures the degree of workflow digitization).

**Are data and evidence routinely used in management and program decisions, or are decisions mostly based on intuition and experience?**

Response Type: Likert scale (0–10) – 0 = "Decisions are almost entirely gut instinct or based on anecdote; data plays little to no role"; 5 = "We occasionally use data or reports for decisions, but it's inconsistent or only for certain departments"; 10 = "Data-driven culture – it's standard to consult data/analysis for most decisions (strategic and operational), and leadership expects data to back up proposals."

Scoring Guidance: Score low if data is rarely part of decision processes; mid if data is used irregularly or by some staff but not others; high if there is a strong norm of evidence-based decision-making. (This reflects the organization's maturity in practice – research shows having a data-driven decision culture facilitates AI adoption[17].)

Category Mapping: Process & Data Practices.

**How standardized and well-documented are your key processes (program delivery, data collection, etc.) across the organization?**

Response Type: Likert scale (0–10) – 0 = "Processes are mostly ad-hoc and vary widely by person or team (little standardization or documentation)"; 5 = "Some processes have standard procedures or documentation, but others are informal"; 10 = "Most important processes are standardized with clear guidelines/SOPs and documentation, ensuring consistency and quality of data collected."

Scoring Guidance: Score higher for more consistency in operations. Low if everything is improvised (which would hinder scaling tech solutions); mid if partial standardization; high if processes are well defined (enabling reliable data flows and easier automation/AI application).

Category Mapping: Process & Data Practices.

(No dedicated open-ended question in this category, but responses from other categories may shed light on process challenges. Evaluators can infer process pain points from context like data quality issues or culture.)

### AI Applications & Risks

Gauges current usage of AI, readiness for AI-specific opportunities, and awareness of risks[18]. This new category focuses on the organization's direct experience with AI and preparedness to manage AI-related challenges (bias, privacy, etc.). Even if an organization hasn't started with AI, these questions identify their level of exploration and caution.

**Has your organization piloted or implemented any AI tools or algorithms in its work?(Examples: AI chatbots for customer support, machine learning for predicting outcomes, AI-based image or text analysis, etc.)**

Response Type: Multiple-choice – e.g.: "No – we have not tried any AI tools yet" (score 0); "Not yet, but we're exploring – AI is on our radar, and we're researching or planning a pilot" (score ~3); "Yes, small pilots – we have experimented with at least one AI tool or pilot project" (score ~7); "Yes, multiple or scaled implementations – we use one or more AI-driven solutions in our operations or programs" (score 10).

Scoring Guidance: Choose the option that best fits and map to score. Even exploratory interest (without action) earns a few points since it shows readiness to engage. Full implementations get highest score.

Category Mapping: AI Applications & Risks (captures current AI adoption level).

**(Conditional follow-up if Q17 answer indicates any AI pilot/use) If yes, what best describes the scale and outcome of your AI initiatives so far?**

Response Type: Multiple-choice – "Pilot in very early stages (limited scope, just testing)"; "Pilot completed, not yet scaled (learned lessons, but not rolled out org-wide)"; "One AI solution integrated into operations (in regular use)"; "Multiple AI solutions fully integrated across different areas." (These can be mapped to an increasing scale, e.g. 3, 5, 8, 10 points respectively, to further refine the AI maturity score.)

Scoring Guidance: This question only appears if the respondent has some AI experience. Use it to boost the AI score for more advanced adoption. For instance, if they have pilots but none scaled, remain in mid-range; if they have at least one AI tool in regular use, that warrants a higher score; if AI is widespread in the org, score maximum.

Category Mapping: AI Applications & Risks (drills deeper into AI use; helps distinguish "AI beginners" from more advanced practitioners).

**Does your organization have any guidelines or protocols to ensure ethical AI use and manage risks like bias, fairness, or data privacy in AI projects?**

Response Type: Multiple-choice – options: "No, we have no specific guidelines for AI ethics" (score 0); "Not yet, but we recognize the need and are looking into it" (score ~3); "In progress – some informal guidelines or ongoing development of policies" (score ~5); "Yes, we have established guidelines or principles for ethical AI use" (score 10).

Scoring Guidance: Score based on the existence of AI-specific ethical oversight. High scores indicate the nonprofit is aware of AI risks and proactively managing them (aligns with best practices of responsible AI governance emphasized in global frameworks[9]). Low scores mean this is a gap that should be addressed especially if they plan to use AI.

Category Mapping: AI Applications & Risks (risk awareness/governance aspect).

**How would you rate your organization's overall awareness and knowledge of AI's potential (and pitfalls) among key decision-makers?**

Response Type: Likert scale (0–10) – 0 = "Very low – key staff/leadership have little understanding of AI and its implications"; 5 = "Moderate – some understanding exists, maybe one or two people follow AI developments, but not broadly informed"; 10 = "High – leadership and relevant staff are well-informed about AI opportunities and risks, stay updated on AI trends, and discuss them when planning."

Scoring Guidance: Score higher if organizational leadership demonstrates knowledge of AI (both benefits and risks). This is somewhat subjective; it can be inferred from earlier answers (like presence of AI projects or policies). Low scores mean AI is not on their radar; mid means sporadic awareness; high means active engagement with AI topics.

Category Mapping: AI Applications & Risks.

**[Open-Ended] What do you see as the most promising opportunity – or the biggest concern – for using AI in your organization's work in the next few years?**

Response Type: Open-ended text.

Scoring Guidance: Qualitative use: This prompt is primarily for reflection and to help tailor recommendations. Rather than a numeric score, categorize the response to identify mindset: e.g. Opportunity-focused (they mention a positive use case they want to pursue, showing enthusiasm), Concern-focused (they emphasize risks or challenges, indicating caution), or Uncertain (not sure what AI means for them yet). Evaluators can use these categories to customize the report narrative – for instance, acknowledging excitement and suggesting next steps if opportunity-focused, or addressing the stated concerns if concern-focused. If a score must be assigned, one approach is to score an opportunity-centric answer slightly higher (indicating proactive interest), and a concern-only answer slightly lower (indicating hesitancy), but in general this question feeds into the qualitative narrative and recommendation engine rather than the quantitative readiness score.

Category Mapping: AI Applications & Risks (contextual, helps interpret the quantitative AI readiness in light of the organization's attitude toward AI).

(Total questions: 20 core questions + 1 conditional sub-question. Each carries a maximum of 10 points, contributing to category scores. The survey typically presents ~20–22 questions to a user (depending on branching), keeping it within a 10-minute completion time as intended[19].)

## Scoring Model and Formula

**Quantitative Scoring:** Each quantitative question is scored on a 0–10 scale (or converted to 0–10). Category scores are calculated as the average of question scores in that category, then normalized to 0–10. For example, if Strategy & Governance has 4 questions totaling 32 points out of 40 max, its category score = 8.0 (which would be 80% readiness in that domain). This approach maintains consistency with the previous 0–10 per question model while allowing flexibility in question formats. Qualitative questions are evaluated using the provided coding guidelines – they can be translated into approximate numeric scores (low/med/high) or used as narrative supplements. Qualitative inputs primarily inform the narrative and recommendations, but if needed for scoring, an evaluator might assign a 0–10 value based on the criteria given for each open-ended prompt.

**Category Weighting:** In computing the overall readiness score, categories are weighted to reflect their relative importance. We maintain a weighted scoring similar to the prior logic (inspired by external indices): e.g. Strategy & Governance ~20%, Data & Tech Infrastructure ~25% (slightly higher, as a solid tech backbone is a prerequisite for AI)[20], People & Skills ~20%, Process & Data Practices ~15%, and AI Applications & Risks ~20%. These weights ensure that a critical weakness in foundational areas (strategy, data, infrastructure) strongly impacts the overall result[21]. The overall readiness score is the weighted aggregate of the category scores (out of 10), which is then mapped to an overall readiness level (see below).

**"Gateway" Conditions:** The scoring logic includes gating rules to temper the overall score if certain fundamentals are lacking[22]. For instance, if an organization indicates it has no data governance policies and very poor data quality (Q2 and Q5 extremely low), the system may cap their maximum overall readiness level at a lower tier, even if other areas scored higher[23]. This reflects that some basic capabilities are non-negotiable for advancing in AI. (Example: if both Strategy/Governance and Data Infrastructure categories are scored in the Nascent range due to critical gaps, the overall readiness will not rise above "Emerging" level, no matter the sum of points[24].) These gateway checks prevent a false sense of maturity when key pillars are missing. Similarly, adaptive follow-ups like Q18 ensure that organizations doing actual AI projects can slightly boost their AI score above those merely planning, improving accuracy in the higher range.

**Readiness Levels:** The tool translates the overall weighted score (0–100% of total points) into one of four maturity levels, adapted from contemporary models[25][26]. The levels are defined as:

**Level 1 – Nascent:** ~0–25% of total points. The organization is at the very beginning of its digital/AI journey. Little to no formal strategy or resources in place; efforts are ad-hoc. (This corresponds to an "AI Skepticism" stage in a 5-stage nonprofit AI maturity model[27] or "Laggard" in some corporate indices.)

**Level 2 – Emerging:** ~25–50% of points. The organization is building fundamental capabilities. Some elements are starting to form (e.g. basic data systems or initial experiments), but a comprehensive or consistent approach is not yet present[28]. (Roughly analogous to early "AI Activation/Experimentation" stages in other frameworks.)

**Level 3 – Operational:** ~50–75% of points. A solid foundation exists and parts of the organization are actively using data/tech. There is a defined strategy, and perhaps one or two successful digital or AI projects, though not yet pervasive. The organization is now focused on integrating and scaling these successes[29].

**Level 4 – Leading:** ~75–100% of points. The organization demonstrates advanced, strategic use of data and AI. There is organization-wide buy-in, multiple use cases in play, and a culture of continuous improvement. This maps to a mature stage of AI readiness (only a small top tier of nonprofits will likely fall here)[30].

Each level comes with an encouraging description in the report to frame the result as a stage in a journey (avoiding terms like "failure"). For example, a Nascent org's result might say "You are at the beginning of your transformation journey, with many opportunities to build a stronger foundation"[31]. We also ensure that if any category is at Level 1 (Nascent), it flags the overall result (e.g. overall might be capped at Level 2 maximum, as noted above)[32] to reinforce addressing that weak spot.

## Reporting Output Templates

When the assessment is completed, the tool generates a results report consisting of an overall readiness level summary and a category-by-category breakdown. The output is written in clear, constructive language, using bullet points under each section for easy reading. Each bullet is tailored to the respondent's score range (level) in that category, providing a narrative that highlights strengths, acknowledges weaknesses, and suggests an improvement direction. The tone is human and strategic – talking to the nonprofit in an encouraging way, as a partner in capacity-building (avoiding overly brief or vague statements). Below are the template narratives for each overall level and for each category at different maturity levels:

**Overall Readiness Level:** (The overall level is determined by the weighted score and gateway logic. Use one of the following bullets accordingly.)

**Nascent – Early in the Journey:** Your organization is at the very early stage of digital and AI readiness. You likely lack a formal strategy and have only ad-hoc or minimal digital tools in place. This is not a bad place to start – it means there are major opportunities to build a strong foundation going forward. Focus now on establishing basic data practices, obtaining leadership buy-in for digital initiatives, and developing a simple roadmap for technology that aligns with your mission. Small first steps can yield significant improvements.[31]

**Emerging – Building the Basics:** Your organization has started laying the groundwork for digital transformation. You might have some basic systems or a few tech-savvy team members, but these efforts are not yet consistent or fully supported across the organization. You're likely experimenting and learning (which is great). To progress, aim to formalize your strategies and policies, invest in staff training, and improve the consistency of data and processes. You are on the right track – the goal now is to turn initial sparks into an organized plan for scaling up.

**Operational – Scaling Up:** Your organization has built a solid foundation in multiple areas of data and tech use. There's a defined strategy and you've seen some successful projects or pilots (for example, a system implementation or a trial of AI/data in a program). The challenge now is to expand and integrate these successes throughout the organization. Focus on breaking down silos, documenting processes, and ensuring everyone is onboard with using data for decision-making. You're approaching a high level of maturity – with continued investment in infrastructure and people, you can move toward truly transforming your operations with AI and digital tools.

**Leading – Strategic & Innovative:** Your organization is among the leaders in leveraging data and AI for social impact. Digital strategy is ingrained at all levels – from leadership vision to daily operations. You have multiple advanced use cases and a culture of continuous improvement. This positions you as a potential role model in the sector. Continue to refine your practices and share your learnings with others. At this stage, focus on maintaining momentum: update your strategies with the latest innovations, ensure ethical safeguards keep pace with your advanced use of AI, and possibly mentor other nonprofits on their journey. Even leaders have room to grow – e.g. exploring cutting-edge AI applications or contributing to sector-wide data collaboration – but you are operating at a very high level already.

### Category Breakdown:

(For each category below, one bullet (or more if needed) is shown, tailored to the organization's score in that category. The bullet highlights where they stand and offers a strategic next step. The report will list each category with its bullet; color-coded icons or labels (e.g. red/yellow/green or level numbers) may accompany to reinforce the score visually[33].)

**Strategy & Governance: Nascent (Low)** – You currently lack a clear digital/data strategy and leadership guidance on these issues. Technology decisions might be ad-hoc and there are no formal policies yet for things like data privacy or AI ethics. This is a foundational gap to address. Consider securing leadership commitment for a basic digital strategy or charter. Even a simple plan or policy statement from the board/executive team can set direction. Opportunity: Start with drafting core principles for data use and get buy-in from top leadership on pursuing digital initiatives.[3][27]

**Emerging (Moderate)** – Your leadership is beginning to engage with digital strategy. Perhaps you have some strategic elements (e.g. a mention of tech in your org's plan or an informal champion at the executive level). You might also have started considering policies (maybe a draft data privacy policy or discussions about AI ethics). To strengthen this, work on formalizing and communicating the strategy. Ensure leadership not only approves but actively promotes a clear vision for how data and AI support your mission. Also, aim to finalize basic governance documents (like a data protection policy). You're building the right habits – now make them official and organization-wide.

**Operational (High)** – You have a documented strategy and strong leadership support for digital innovation. This likely includes some policies for responsible tech use and regular discussions at the leadership or board level about progress. This is a significant strength for your organization. The key is now ensuring this strategy remains a "living" guide. Regularly update your digital/AI strategy to keep pace with new opportunities, and continue strengthening governance (e.g. perhaps establish an internal committee on digital ethics or have board oversight for major tech projects). Your strong governance will enable safe and mission-aligned growth in tech use – keep nurturing that strategic oversight.

**Leading (Very High)** – Strategic vision and governance are clear highlights for you. Leadership not only supports but likely pioneers the use of data/AI in your mission, and robust governance frameworks (policies, ethical guidelines, accountability structures) are in place. You treat data and AI as strategic assets, guided by principles of trust and transparency. Going forward, focus on sustaining your strategic edge: continually refine your strategies in light of evolving technology and social context (for example, update ethical guidelines as AI advances, or integrate digital strategy deeply into your organizational strategy). You might also share your governance models with peers or collaborate in sector-wide initiatives for AI ethics, given your maturity in this area.

**Data & Technology Infrastructure: Nascent (Low)** – Your technology and data infrastructure are basic or outdated, which could significantly hold back digital transformation. Data may be siloed in spreadsheets (or not digitized at all), and there might be reliability issues (e.g. poor internet, old computers). It's hard to leverage AI or even advanced analytics without a stronger foundation. The immediate priority is to invest in basic IT capacity and data management. Focus on consolidating data into a central system if possible, improving data quality (even simple data cleaning routines), and ensuring staff have at least adequate tools and internet access. You might start with low-cost solutions (many cloud tools offer nonprofit discounts) to gradually modernize your infrastructure[34].

**Emerging (Moderate)** – You have some infrastructure pieces in place – for instance, maybe a donor database or a cloud drive – but there are gaps. Data exists but might not be fully integrated or consistently clean, and not all useful tools are in use yet. You're likely aware of these issues. To advance, aim for better integration and reliability: connect systems where feasible (or adopt a platform that brings multiple functions together), implement regular data backup and quality checks, and introduce tools that can automate manual work. Upgrading infrastructure doesn't have to be expensive: prioritize the areas that directly support your strategic goals (e.g. if program data is key, focus on a good program database first). Strengthening your data architecture now will pay off as you try more advanced analytics later.

**Operational (High)** – Your data and tech infrastructure is fairly strong. You likely have solid databases or software for core operations and decent hardware and connectivity for staff. Data is reasonably well organized and accessible to those who need it. To build on this strength, look at optimizing and scaling: ensure all departments that need to share data are integrated into your systems (eliminate any remaining silos), and consider more advanced tools if gaps exist (for example, if you have good databases, maybe now add a business intelligence dashboard or improve your data security and privacy measures to an industry standard). You're at a point where fine-tuning infrastructure – and keeping it up-to-date – will support the innovative projects you want to tackle next.

**Leading (Very High)** – You have a robust, modern infrastructure supporting your work. Systems are integrated, data is high-quality and readily accessible, and staff have the tools and connectivity they need. This provides a powerful platform for AI and digital innovation. The goal now is sustainability and edge: keep your infrastructure current (plan for updates or new tech as needed), and explore if emerging technologies (e.g. better cloud services, data warehousing, or AI platforms) can further enhance efficiency. Given your advanced stage, also double-check that your infrastructure aligns with best practices in security and scalability – as a leader, protecting your data assets is crucial. Overall, you're well-positioned to exploit advanced tech; just continue maintaining and strategically growing this backbone.

**People & Skills: Nascent (Low)** – Right now, limited staff capacity or tech skills are a bottleneck. Few people, if any, have data analysis or AI knowledge, and there may have been no formal training opportunities. The organizational culture might also be cautious about tech. This is very common in early-stage nonprofits – the key is to start building a learning culture. Identify interested staff or volunteers who can become champions after some training. Look for free or low-cost workshops (many exist for nonprofits) to get your team's feet wet with data skills[14]. Also, encourage leadership to create time and incentives for staff to experiment with simple tech tools. By improving the human capacity (even gradually), you'll unlock the ability to use better tech down the line.

**Emerging (Moderate)** – Your team has some digital skills or at least openness to learning. Perhaps a few individuals are data-savvy or you've done one-off trainings, but skills aren't widespread yet. You might not have dedicated data roles, but you know who the "go-to" person is for tech help. The culture is not opposed to tech, but maybe not fully embracing it either. To improve, you should systematize capacity-building: consider setting up a more regular training schedule (quarterly workshops or online courses), and try to formalize roles (even part-time) for data/IT responsibilities. Also, nurture a more innovative culture by celebrating small tech wins and encouraging cross-team sharing of new ideas. Your goal is to move from a few pockets of skills to a situation where most staff have at least baseline data literacy, and a core team leads the charge on advanced projects.

**Operational (High)** – You've developed significant people capacity for digital work. Likely many staff have gone through training, and you might have one or more dedicated data/tech specialists or teams. The culture is increasingly data-driven and open to new tools – staff are not afraid to adopt software that helps them. Continue to leverage this strength by deepening and broadening skills: support advanced training for your data specialists (so they can implement AI solutions, for example) and provide ongoing learning for general staff to keep everyone up to date. It could also be time to look at talent pipelines: if budget allows, hire new talent with specialized skills or engage pro bono experts for specific projects. Given your strong culture, consider implementing an internal "innovation challenge" or sandbox where staff can propose and test tech ideas. Maintaining momentum in skills development will ensure your organization keeps progressing.

**Leading (Very High)** – People and culture are a strong asset for your organization's innovation goals. You have skilled staff (maybe a data science team or an innovation unit) and a broad base of digitally literate employees. Continuous learning is likely part of your DNA – staff actively update their skills and share knowledge, and leadership supports this fully. The culture probably encourages smart risk-taking and creative use of data/AI in solving problems. At this stage, your people strategy might involve retaining and inspiring talent: provide pathways for your experts to grow and lead within the sector (so they don't leave for other opportunities), and keep an eye on diversity and inclusion in your tech teams (bringing in varied perspectives can spur more innovative AI solutions). You could also mentor other organizations or contribute your expertise to sector-wide capacity building, which often also feeds back new ideas to your team. In short, keep doing what you're doing – your investment in people is clearly paying off.

**Process & Data Practices: Nascent (Low)** – Your organizational processes are largely non-digitized and informal. Many workflows might be manual, and data isn't consistently collected or used. Decisions are probably made on experience and intuition, with little support from data analysis. This makes it hard for technology to take root. The immediate next step is to introduce simple digital processes and data habits. For example, start using basic tools (if you haven't) like an Excel-based tracker or a simple database for key activities to generate data. Encourage staff to capture data about what they do, and use that in team meetings. Begin standardizing one or two core processes (like how you track beneficiaries or donations) so that you can measure and learn. By formalizing processes and gathering data, you create the raw material needed for any AI or analytics in the future. It's about building a data-driven mindset step by step.

**Emerging (Moderate)** – You have partially digitized processes and some use of data in decision-making, but it's inconsistent. Maybe certain departments use tools (e.g. finance uses accounting software, programs might use a simple M&E form), but others lag. You might look at data occasionally (like an annual report or survey) but it's not embedded in everyday decisions. The goal now is to streamline and integrate these practices: pick a few high-value areas and fully digitize them (for example, adopt a proper CRM if you haven't, or use an online project management tool). Also, work on creating regular routines for data use – e.g. monthly dashboards or review meetings where data is discussed. Standardize procedures across teams so everyone is following best practices, not each doing their own thing. As processes align and data becomes a common language, you'll see more informed decisions and readiness for advanced tools.

**Operational (High)** – Your processes are mostly digitized and data-informed. It sounds like many workflows use software, and there's a habit of using data when planning or evaluating programs. Some processes are well standardized across the org. This gives you a strong platform for efficiency and scaling. To push this further, focus on refinement and optimization: ensure all critical processes (not just most) have a digital workflow and that these systems talk to each other (integrate data from various processes for a holistic view). You might implement organization-wide dashboards or real-time monitoring tools to enhance decision-making. Also, continue to foster the practice of evidence-based management – perhaps set specific targets or KPIs and track them openly to drive performance. You're doing well; the aim is to make data use second nature in every department and to fine-tune processes for even better data quality (which, in turn, feeds AI efforts).

**Leading (Very High)** – Integrated, data-driven processes are a hallmark of your operations. Virtually all key workflows are digitized, and you have an established culture of using data at every level (from frontline staff adjusting programs based on data, to leadership strategic planning with data insights). Processes are standardized and continuously improved, possibly using advanced techniques (you might be leveraging AI or automation to optimize processes already). At this top level, your focus should be on continuous improvement and innovation in processes: explore cutting-edge practices like real-time data analytics for quick feedback loops, process mining to find inefficiencies, or AI-driven process automation where appropriate. Also, ensure that as you innovate, you maintain flexibility – highly mature organizations sometimes face bureaucracy, so keep encouraging experimentation. You might pilot new approaches (like predictive analytics for decision support) given your solid foundation. Essentially, you have the freedom to try frontier technologies in your processes because you trust the data and systems underneath them.

**AI Applications & Risks: Nascent (Low)** – Your organization is not yet using AI, and that's perfectly okay – many are in this boat. Perhaps AI hasn't been on your radar, or you feel you're not ready. You likely also don't have any AI-specific policies or knowledge in place yet (since you're not using it). The key is to start with awareness and small experiments. AI is becoming more accessible; consider learning about simple use cases relevant to your mission (for example, could a basic chatbot help answer common questions, or might predictive analytics help target services?). Also, even if you're not using AI, begin thinking about the data and ethics side: what data do you have that could power AI in the future, and how will you ensure privacy and fairness? Starting to pilot something small when you're ready – with proper guidance – will help you learn. The fact that you're assessing this means you're on the way to moving past the skeptic stage.[35][36]

**Emerging (Moderate)** – You are aware of AI and maybe exploring possibilities. Perhaps you haven't fully implemented anything yet, but you might be planning a pilot or have done a proof-of-concept on a small scale. You may have begun discussing AI risks, although formal policies might still be in progress. This is an exciting stage – to advance, try to complete a pilot project and develop basic AI guidelines. For example, if you've identified a use case (say, an AI tool for fundraising or analyzing program data), run a controlled experiment and evaluate the results. Use that experience to create simple guidelines: how will you handle data, what ethical issues came up, etc. Also, invest in learning: ensure key team members get some AI literacy (there are nonprofit-specific AI workshops and courses). You're on the cusp of doing something real with AI – with careful planning and ethical guardrails, you can move to actual implementation.

**Operational (High)** – Your organization has experimented with AI and even implemented at least one solution. You're ahead of many – perhaps you have a chatbot running, or use machine learning for analysis in a project. You likely have learned lessons from pilots and started integrating AI into operations in a meaningful way. You also appear to be aware of AI risks and may have some policies or at least informal practices to ensure ethical use. The path forward is to scale and formalize: expand successful AI applications to more programs or departments where they add value, and formalize your AI governance (perhaps a committee to oversee AI projects or written guidelines for staff using AI tools). Also, keep building AI literacy across the organization so that you have a deeper bench of people who can manage and improve AI systems. You're doing well – focus on strategic scaling rather than just experimenting.